kairosdb: {

	# To set the traffic type allowed for the server, change the kairosdb.server.type entry to INGEST, QUERY, or DELETE,
	# or a comma delimited list of two of them to enable different sets of API methods.
	# The default setting is ALL, which will enable all API methods.
	#server.type: "ALL"

	# Properties that start with kairosdb.service are services that are started
	# when kairos starts up.  You can disable services in your custom
	# kairosdb.properties file by setting the value to <disabled> ie
	# kairosdb.service.telnet=<disabled>
	service.telnet: "org.kairosdb.core.telnet.TelnetServerModule"
	telnetserver: {
		port: 4242
		address: "0.0.0.0"
		max_command_size: 1024
	}

	#===============================================================================
	service.http = org.kairosdb.core.http.WebServletModule
	jetty: {
		# Set to 0 to turn off HTTP port
		port: 8080
		address: "0.0.0.0"
		static_web_root: "webroot"

		# To enable SSL uncomment the following lines and specify the path to the keyStore and its password and port
		#ssl: {
		#port: 443
		#protocols: "TLSv1, TLSv1.1, TLSv1.2"
		#cipherSuites: "TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA"
		#keystore.path=
		#keystore.password=
		#}

		#To enable http basic authentication uncomment the following lines and specify
		#the user name and password for authentication.
		#basic_auth.user: ""
		#basic_auth.password: ""

		# To enable thread pooling uncomment the following lines and specify the limits
		#threads.queue_size: 6000
		#threads.min: 1000
		#threads.max: 2500
		#threads.keep_alive_ms: 10000
	}

	#===============================================================================
	# Each factory must be bound in a guice module.  The definition here defines what
	# protocol data type the factory services.
	datapoints.factory: {
		# Default data point implementation for long - class must implement LongDataPointFactory
		long: "org.kairosdb.core.datapoints.LongDataPointFactoryImpl"
		# Default data point implementation for double - class must implement DoubleDataPointFactory
		double: "org.kairosdb.core.datapoints.DoubleDataPointFactoryImpl"
		string: "org.kairosdb.core.datapoints.StringDataPointFactory"
	}

	#===============================================================================
	service.reporter = org.kairosdb.core.reporting.MetricReportingModule
	reporter: {
		# Uses Quartz Cron syntax - default is to run every minute
		schedule: "0 */1 * * * ?"
		# TTL to apply to all kairos reported metrics
		ttl: 0
	}

	#===============================================================================
	#Configure the datastore

	service.datastore: "org.kairosdb.datastore.h2.H2Module"
	#service.datastore: "org.kairosdb.datastore.cassandra.CassandraModule"
	#service.datastore: "org.kairosdb.datastore.remote.RemoteModule"

	datastore.concurrentQueryThreads: 5

	datastore.h2.database_path: "build/h2db"

	datastore.cassandra: {
		#For a single metric query this dictates the number of simultaneous cql queries
		#to run (ie one for each partition key of data).  The larger the cluster the higher you may want
		#this number to be.
		simultaneous_cql_queries: 20

		# query_reader_threads is the number of threads to use to read results from
		# each cql query.  You may want to change this number depending on your environment
		query_reader_threads: 6


		# Sets the percentage to check against for row key query failure tolerance.
		# Can be set from 0.0 to 1.0. Default is 0.0.
		query_failure_tolerance=0.0

		# When set, the query_limit will prevent any query reading more than the specified
		# number of data points.  When the limit is reached an exception is thrown and an
		# error is returned to the client.  Set this value to 0 to disable (default)
		#query_limit: 10000000

		//Todo this is wrong
		#Size of the row key cache size.  This can be monitored by querying
		#kairosdb.datastore.write_size and filtering on the tag buffer = row_key_index
		#Ideally the data written to the row_key_index should stabilize to zero except
		#when data rolls to a new row
		row_key_cache_size: 50000
		string_cache_size: 50000

		#the time to live in seconds for datapoints. After this period the data will be
		#deleted automatically. If not set the data will live forever.
		#TTLs are added to columns as they're inserted so setting this will not affect
		#existing data, only new data.
		#datapoint_ttl: 31536000

		write_cluster: {
			# name of the cluster as it shows up in client specific metrics
			name: "write_cluster"
			keyspace: "kairosdb"
			replication: "{'class': 'SimpleStrategy','replication_factor' : 1}"
			cql_host_list: ["localhost"]
			//cql_host_list: ["kairos01", "kairos02", "kairos03"]

			# Set this if this kairosdb node connects to cassandra nodes in multiple datacenters.
			# Not setting this will select cassandra hosts using the RoundRobinPolicy, while setting this will use DCAwareRoundRobinPolicy.
			#local_dc_name: "<local dc name>"

			#Control the required consistency for cassandra operations.
			#Available settings are cassandra version dependent:
			#http://www.datastax.com/documentation/cassandra/2.0/webhelp/index.html#cassandra/dml/dml_config_consistency_c.html
			read_consistency_level: "ONE"
			write_consistency_level: "QUORUM"

			#The number of times to retry a request to C* in case of a failure.
			request_retry_count: 2

			connections_per_host: {
				local.core: 5
				local.max: 100

				remote.core: 1
				remote.max: 10
			}

			# If using cassandra 3.0 or latter consider increasing this value
			max_requests_per_connection: {
				local: 128
				remote: 128
			}

			max_queue_size: 500

			#for cassandra authentication use the following
			#auth.[prop name]=[prop value]
			#example:
			#auth.user_name=admin
			#auth.password=eat_me

			# Set this property to true to enable SSL connections to your C* cluster.
			# Follow the instructions found here: http://docs.datastax.com/en/developer/java-driver/3.1/manual/ssl/
			# to create a keystore and pass the values into Kairos using the -D switches
			use_ssl: false
		}

		# Rename this to read_clusters in order for it to be used
		read_clusters_not: [
			{
				name: "read_cluster"
				keyspace: "kairosdb"
				replication: "{'class': 'SimpleStrategy','replication_factor' : 1}"
				//cql_host_list: ["kairos01", "kairos02", "kairos03"]
				cql_host_list: ["localhost"]
				#local_dc_name: "<local dc name>
				read_consistency_level: "ONE"
				write_consistency_level: "QUORUM"

				connections_per_host: {
					local.core: 5
					local.max: 100
					remote.core: 1
					remote.max: 10
				}

				max_requests_per_connection: {
					local: 128
					remote: 128
				}

				max_queue_size: 500
				use_ssl: false

				# Start and end date are optional configuration parameters
				# The start and end date set bounds on the data in this cluster
				# queries that do not include this time range will not be sent
				# to this cluster.
				start_time: "2001-07-04T12:08-0700"
				end_time: "2001-07-04T12:08-0700"
			}
			]


	}

	#===============================================================================
	# Remote datastore properties
	# Load the RemoteListener modules instead of RemoteDatastore if you want to
	# fork the flow of data.  This module allows you to continue writting to your
	# configured Datastore as well as send data on to a remote Kairos cluster
	# Sample use case is to run clusters in parallel before migrating to larger cluster
	# Cannot be used in conjunction with the RemoteModule
	#service.remote: "org.kairosdb.datastore.remote.ListenerModule"

	datastore.remote: {
		# Location to store data locally before it is sent off
		data_dir: "."
		remote_url: "http://10.92.1.41:8080"

		# quartz cron schedule for sending data (currently set to 30 min)
		schedule: "0 */30 * * * ?"

		# delay the sending of data for a random number of seconds.
		# this prevents all remote kairos nodes from sending data at the same time
		# the default of 600 means the data will be sent every half hour plus some some
		# delay up to 10 minutes.
		random_delay: 0

		# Optional prefix filter for remote module. If present, only metrics that start with the
		# values in this comma-separated list are forwarded on.
		#prefix_filter: ""
	}

	#===============================================================================
	#Uncomment this line to require oauth connections to http server
	#service.oauth: "org.kairosdb.core.oauth.OAuthModule"

	#OAuth consumer keys and secrets in the form
	#oauth.consumer: {
	# [consumer key]: "[consumer secret]"
	#}

	#===============================================================================
	# Determines if cache files are deleted after being used or not.
	# In some cases the cache file can be used again to speed up subsequent queries
	# that query the same metric but aggregate the results differently.
	query_cache.keep_cache_files: false

	# Cache file cleaning schedule. Uses Quartz Cron syntax - this only matters if
	# keep_cache_files is set to true
	query_cache.cache_file_cleaner_schedule: "0 0 12 ? * SUN *"

	#By default the query cache is located in kairos_cache under the system temp folder as
	#defined by java.io.tmpdir system property.  To override set the following value
	#query_cache.cache_dir: ""

	#===============================================================================
	# Log long running queries, set this to true to record long running queries
	# into kairos as the following metrics.
	# kairosdb.log.query.remote_address - String data point that is the remote address
	#   of the system making the query
	# kairosdb.log.query.json - String data point that is the query sent to Kairos
	log.queries: {
		enable: false

		# Time to live to apply to the above metrics.  This helps limit the mount of space
		# used for storing the query information
		ttl: 86400

		# Time in seconds.  If the query request time is longer than this value the query
		# will be written to the above metrics
		greater_than: 60
	}

	# When set to true the query stats are aggregated into min, max, avg, sum, count
	# Setting to true will also disable the above log feature.
	# Set this to true on Kairos nodes that receive large numbers of queries to save
	# from inserting data witch each query
	queries.aggregate_stats = false

	# If a tag filter value begins with this string the remaining is considered a
	# regex to match against those tag values.  ei {"host": "regex:server1[0-2]"}
	# matches host tag values server10, server11 and server12
	# set the value to an empty string to disable
	queries.regex_prefix = "regex:"

	#===============================================================================
	# Health Checks
	service.health: "org.kairosdb.core.health.HealthCheckModule"

	#Response code to return from a call to /api/v1/health/check
	#Some load balancers want 200 instead of 204
	health.healthyResponseCode: 204

	#===============================================================================
	#Ingest queue processor
	# The MemoryQueueProcessor keeps everything in memory before batching to
	# cassandra and blocks when the queue is full
	# The FileQueueProcessor uses a hybrid memory queue and a file backed queue
	# Data is placed in both memory and in the file queue before a client response
	# is sent.  Data is read from file only when the lag is greater than what the
	# memory queue can hold

	queue_processor: {
		#class: "org.kairosdb.core.queue.MemoryQueueProcessor"
		class: "org.kairosdb.core.queue.FileQueueProcessor"

		# The number of data points to send to Cassandra
		# For the best performance you will want to set this to 10000 but first
		# you will need to change the following values in your cassandra.yaml
		# batch_size_warn_threshold_in_kb: 50
		# batch_size_fail_threshold_in_kb: 70
		# You may need to adjust the above numbers to fit your insert patterns.
		# The CQL batch has a hard limit of 65535 items in a batch, make sure to stay
		# under this as a single data point can generate more than one insert into Cassandra
		# You will want to multiply this number by the number of hosts in the Cassandra
		# cluster.  A batch is pulled from the queue and then divided up depending on which
		# host the data is destined for.
		batch_size: 200

		# If the queue doesn't have at least this many items to process the process thread
		# will pause for .5 seconds to wait for more before grabbing data from the queue.
		# This is an attempt to prevent chatty inserts which can cause extra load on
		# Cassandra
		min_batch_size: 100

		# If the number of items in the process queue is less than {min_batch_size} the
		# queue processor thread will wait this many milliseconds before flushing the data
		# to C*.  This is to prevent single chatty inserts.  This only has effect when
		# data is trickling in to Kairos.
		min_batch_wait: 500

		# The size (number of data points) of the memory queue
		# In the case of FileQueueProcessor:
		# Ingest data is written to the memory queue as well as to disk.  If the system gets
		# behind the memory queue is overrun and data is read from disk until it can
		# catch up.
		# In the case of MemoryQueueProcessor it defines the size of the memory queue.
		memory_queue_size: 100000

		# The number of seconds before checkpointing the file backed queue.  In the case of
		# a crash the file backed queue is read from the last checkpoint
		# Only applies to the FileQueueProcessor
		seconds_till_checkpoint: 90

		# Path to the file backed queue
		# Only applies to the FileQueueProcessor
		queue_path: "queue"

		# Page size of the file backed queue 50Mb
		# Only applies to the FileQueueProcessor
		page_size: 52428800
	}

	#Number of threads allowed to insert data to the backend
	#CassandraDatastore is the only use of this executor
	ingest_executor.thread_count = 10



	host_service_manager: {
		check_delay_time_millseconds: 30000
		inactive_time_seconds: 30
	}

  #===============================================================================
  # Roll-ups
	#service.rollups=org.kairosdb.rollup.RollUpModule

	# How often the Roll-up Manager queries for new or updated roll-ups\
	#rollups: {
	#	server_assignment {
	#			check_update_delay_millseconds = 10000
	#		}
	#	}
	#===============================================================================


	#===============================================================================
	#Demo and stress modules

	# The demo module will load one years of data into kairos.  The code goes back
	# one year from the present and inserts data every minute.  The data makes a
	# pretty little sign wave.

	#service.demo: "org.kairosdb.core.demo.DemoModule"
	demo: {
		metric_name: "demo_data"
		# number of rows = number of host tags to add to the data. Setting the number_of_rows
		# to 100 means 100 hosts reporting data every minutes, each host has a different tag.
		number_of_rows: 100
		ttl: 0
	}


	# This just inserts data as fast as it can for the duration specified.  Good for
	# stress testing your backend.  I have found that a single Kairos node will only
	# send about 500k/sec because of a limitation in the cassandra client.

	#service.blast: "org.kairosdb.core.blast.BlastModule"
	# The number_of_rows translates into a random number between 0 and number_of_rows
	# that is added as a tag to each data point.  Trying to simulate a even distribution
	# data in the cluster.
	blast: {
		number_of_rows: 1000
		duration_seconds: 30
		metric_name: "blast_load"
		ttl: 600
	}
}

